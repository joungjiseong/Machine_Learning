from tensorflow.keras.models import load_model
import pickle
import pandas as pd
import numpy as np
import re
from tensorflow.keras.preprocessing.sequence import pad_sequences

# í•¨ìˆ˜ ì •ì˜
def count_special_characters(text):
    return len(re.findall(r'[^\w\s]', text))

def count_emojis(text):
    emoji_pattern = re.compile(
        "[\U0001F600-\U0001F64F"
        "\U0001F300-\U0001F5FF"
        "\U0001F680-\U0001F6FF"
        "\U0001F700-\U0001F77F"
        "\U0001F780-\U0001F7FF"
        "\U0001F800-\U0001F8FF"
        "\U0001F900-\U0001F9FF"
        "\U0001FA00-\U0001FA6F"
        "\U0001FA70-\U0001FAFF"
        "\U00002702-\U000027B0"
        "]+", flags=re.UNICODE)
    return len(emoji_pattern.findall(text))

# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
loaded_model = load_model('spamModel.h5')
with open('tokenizer.pkl', 'rb') as file:
    tokenizer = pickle.load(file)

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
test_data = pd.DataFrame({
    'MailName': [
        "í• ì¸! ì§€ê¸ˆ ë°”ë¡œ êµ¬ë§¤í•˜ì„¸ìš” ğŸ‰",              # ê´‘ê³ ì„± ì œëª©
        "ê¸´ê¸‰: ê³„ì • ë³´ì•ˆ ê²½ê³  âš ï¸",                  # ìŠ¤íŒ¸ìœ¼ë¡œ ì˜ì‹¬ë  ìˆ˜ ìˆëŠ” ì œëª©
        "ë‹¹ì²¨ ì¶•í•˜ë“œë¦½ë‹ˆë‹¤! â˜˜ï¸ğŸ",                   # ìŠ¤íŒ¸ì„± ì œëª©
        "ì•ˆë…•í•˜ì„¸ìš”, í”„ë¡œì íŠ¸ ê´€ë ¨ ë¬¸ì˜ë“œë¦½ë‹ˆë‹¤.",       # ì •ìƒì ì¸ ì œëª©
        "ğŸ“¢ ì¤‘ìš”í•œ ê³µì§€ì‚¬í•­ í™•ì¸ ë°”ëë‹ˆë‹¤!",            # í˜¼í•©í˜• ì œëª©
        "ë¬´ë£Œ ìƒí’ˆê¶Œì„ ë“œë¦½ë‹ˆë‹¤ ğŸŸï¸",                 # ìŠ¤íŒ¸ ì˜ì‹¬
        "ì´ë²ˆ ì£¼ íšŒì˜ ì¼ì • í™•ì¸ ë¶€íƒë“œë¦½ë‹ˆë‹¤.",         # ì •ìƒ
        "ğŸš€ ë¹ ë¥¸ ëŒ€ì¶œ ìŠ¹ì¸ì„ ë„ì™€ë“œë¦½ë‹ˆë‹¤!",            # ìŠ¤íŒ¸ ì˜ì‹¬
        "ì¡°ìš©íˆ ì‚¬ë¼ì§„ ì „ì„¤, ê·¸ ë¹„ë°€ì„ ë°í˜€ë“œë¦½ë‹ˆë‹¤!",    # ê´‘ê³ ì„± ì œëª©
        "ì•ˆì „í•˜ê³  ë¹ ë¥¸ ë¹„ë°€ë²ˆí˜¸ ë³µêµ¬ ì•ˆë‚´ ì´ë©”ì¼ì…ë‹ˆë‹¤."    # ì •ìƒì ì¸ ì œëª©
    ],
    'special_char_count': [
        count_special_characters("í• ì¸! ì§€ê¸ˆ ë°”ë¡œ êµ¬ë§¤í•˜ì„¸ìš” ğŸ‰"),
        count_special_characters("ê¸´ê¸‰: ê³„ì • ë³´ì•ˆ ê²½ê³  âš ï¸"),
        count_special_characters("ë‹¹ì²¨ ì¶•í•˜ë“œë¦½ë‹ˆë‹¤! â˜˜ï¸ğŸ"),
        count_special_characters("ì•ˆë…•í•˜ì„¸ìš”, í”„ë¡œì íŠ¸ ê´€ë ¨ ë¬¸ì˜ë“œë¦½ë‹ˆë‹¤."),
        count_special_characters("ğŸ“¢ ì¤‘ìš”í•œ ê³µì§€ì‚¬í•­ í™•ì¸ ë°”ëë‹ˆë‹¤!"),
        count_special_characters("ë¬´ë£Œ ìƒí’ˆê¶Œì„ ë“œë¦½ë‹ˆë‹¤ ğŸŸï¸"),
        count_special_characters("ì´ë²ˆ ì£¼ íšŒì˜ ì¼ì • í™•ì¸ ë¶€íƒë“œë¦½ë‹ˆë‹¤."),
        count_special_characters("ğŸš€ ë¹ ë¥¸ ëŒ€ì¶œ ìŠ¹ì¸ì„ ë„ì™€ë“œë¦½ë‹ˆë‹¤!"),
        count_special_characters("ì¡°ìš©íˆ ì‚¬ë¼ì§„ ì „ì„¤, ê·¸ ë¹„ë°€ì„ ë°í˜€ë“œë¦½ë‹ˆë‹¤!"),
        count_special_characters("ì•ˆì „í•˜ê³  ë¹ ë¥¸ ë¹„ë°€ë²ˆí˜¸ ë³µêµ¬ ì•ˆë‚´ ì´ë©”ì¼ì…ë‹ˆë‹¤.")
    ],
    'emoji_count': [
        count_emojis("í• ì¸! ì§€ê¸ˆ ë°”ë¡œ êµ¬ë§¤í•˜ì„¸ìš” ğŸ‰"),
        count_emojis("ê¸´ê¸‰: ê³„ì • ë³´ì•ˆ ê²½ê³  âš ï¸"),
        count_emojis("ë‹¹ì²¨ ì¶•í•˜ë“œë¦½ë‹ˆë‹¤! â˜˜ï¸ğŸ"),
        count_emojis("ì•ˆë…•í•˜ì„¸ìš”, í”„ë¡œì íŠ¸ ê´€ë ¨ ë¬¸ì˜ë“œë¦½ë‹ˆë‹¤."),
        count_emojis("ğŸ“¢ ì¤‘ìš”í•œ ê³µì§€ì‚¬í•­ í™•ì¸ ë°”ëë‹ˆë‹¤!"),
        count_emojis("ë¬´ë£Œ ìƒí’ˆê¶Œì„ ë“œë¦½ë‹ˆë‹¤ ğŸŸï¸"),
        count_emojis("ì´ë²ˆ ì£¼ íšŒì˜ ì¼ì • í™•ì¸ ë¶€íƒë“œë¦½ë‹ˆë‹¤."),
        count_emojis("ğŸš€ ë¹ ë¥¸ ëŒ€ì¶œ ìŠ¹ì¸ì„ ë„ì™€ë“œë¦½ë‹ˆë‹¤!"),
        count_emojis("ì¡°ìš©íˆ ì‚¬ë¼ì§„ ì „ì„¤, ê·¸ ë¹„ë°€ì„ ë°í˜€ë“œë¦½ë‹ˆë‹¤!"),
        count_emojis("ì•ˆì „í•˜ê³  ë¹ ë¥¸ ë¹„ë°€ë²ˆí˜¸ ë³µêµ¬ ì•ˆë‚´ ì´ë©”ì¼ì…ë‹ˆë‹¤.")
    ]
})

X_test = test_data[['MailName', 'special_char_count', 'emoji_count']]
max_len = 64

# ì „ì²˜ë¦¬
X_test_encoded = tokenizer.texts_to_sequences(X_test['MailName'])
X_test_padded = pad_sequences(X_test_encoded, maxlen=max_len)
X_test_special = np.hstack([X_test[['special_char_count', 'emoji_count']].values])

# ì˜ˆì¸¡ ì‹¤í–‰
predictions = loaded_model.predict([X_test_padded, X_test_special])
for i, pred in enumerate(predictions):
    print(f"ì´ë©”ì¼: {X_test['MailName'].iloc[i]}")
    print(f"ì˜ˆì¸¡ í™•ë¥ : {pred[0]:.4f}")
    print("ì˜ˆì¸¡ ê²°ê³¼: ìŠ¤íŒ¸" if pred[0] < 0.2 else "ì˜ˆì¸¡ ê²°ê³¼: ì •ìƒ")
